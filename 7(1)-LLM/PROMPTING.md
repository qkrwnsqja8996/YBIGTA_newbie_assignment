## LLM 보고서
### 28기 박준범

## 1. Direct Prompting, CoT Prompting, My Prompting의 0 shot, 3 shot, 5 shot 정답률 비교
| | 0 shot | 3 shot | 5 shot |
|---|--------|---|---|
|Direct| 0.80   | 0.76 | 0.74|
|CoT| 0.72   | 0.80 | 0.84 |
|My| 0.72   | 0.86 | 0.82|

## 2. CoT Prompting이 Direct Prompting에 비해 왜 좋을 수 있는지
Direct Prompting은 모델에게 문제와 정답만을 예시로 제공하여 중간 추론 과정을 생략한 채 곧바로 결과를 출력하게 만드는 방식이다. 이 방식은 연산이 적고 직관적인 문제에서는 효율적일 수 있으나, 복잡한 문제에서는 모델이 문제의 본질을 파악하기보다 과거 데이터의 패턴에만 의존하게 만든다. 그 결과, 난이도가 높은 과제에서는 논리적 비약이나 연산 오류를 범할 가능성이 높아진다는 한계가 있다.
반면, Chain-of-Thought (CoT) Prompting은 '문제-논리-정답'의 구조를 통해 모델이 문제 해결을 위한 '사고 과정'을 거치도록 유도한다. CoT는 복잡한 문제를 관리 가능한 작은 단위로 단계적 분해하여 처리함으로써, 모델이 인간처럼 체계적으로 문제에 접근하게 한다. 단순히 정답을 모방하는 것이 아니라 명시적인 중간 추론 과정을 서술하게 하여, 모델이 내재된 추론 능력을 극대화하고 계산의 정합성을 확보하도록 돕는다.
이러한 사고의 흐름은 논리적 오류를 스스로 바로잡는 검증 단계 역할을 수행하여, 단순 암기가 아닌 논리적 이해를 바탕으로 답변을 생성하게 한다. 결과적으로 CoT는 Direct Prompting보다 일관된 논리 전개와 높은 정확도를 보장하며, 특히 수학 문제와 같이 복잡한 추론이 필요한 영역에서 더 신뢰할 수 있는 성능을 발휘한다.

## 3. 본인이 작성한 프롬프트 기법이 CoT에 비해서 왜 더 좋을 수 있는지

My Prompting은 CoT의 논리적 사고 흐름을 계승하지만 단일 페르소나와 검증 단계를 도입하여 추론의 정확성을 구조적으로 보완한 방식이다. 이 방식은 모델에게 '수학 대회 코치'와 같은 구체적인 역할을 부여함으로써, 단순히 문제를 푸는 것을 넘어 더욱 엄격한 논리 기준을 적용하도록 유도한다. 특히, 답안 도출 직전에 계산 과정을 재확인하는 단계를 강제함으로써, 복잡한 연산 과정에서 발생할 수 있는 우발적인 오류를 사전에 차단하게끔 설계되었다. 이러한 전략은 모델이 단순한 문제 풀이를 넘어 전문가 수준의 책임감을 갖게 함으로써 논리적 정합성을 극대화했고 결과적으로 3-shot 기준 86%라는 가장 높은 정답률을 달성하여 기존 CoT 방식보다 더욱 신뢰할 수 있는 성능을 입증하였다.